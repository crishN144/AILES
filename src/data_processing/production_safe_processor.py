#!/usr/bin/env python3
"""
Enhanced AILES XML-to-Dataset Pipeline
Addresses all critical issues and adds new enhancements:
- Increased pair yield (max_chunks = 20)
- Balanced task distribution with quota relaxation
- RAG integration for regenerative pairs
- Fixed missing responses validation
- Factuality logging in tracker
- Production-ready for 4,611 files targeting ~17,000 pairs
"""

import os
import json
import random
import re
import time
import signal
from collections import defaultdict, Counter
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
import pandas as pd
import pickle

# Required imports
try:
    from lxml import etree as LET
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from sentence_transformers import SentenceTransformer
    import faiss
    import numpy as np
    DEPENDENCIES_AVAILABLE = True
    RAG_AVAILABLE = True
except ImportError as e:
    print(f"Missing dependencies: {e}")
    DEPENDENCIES_AVAILABLE = False
    RAG_AVAILABLE = False

# Configuration
CORPUS_DIR = "/users/bgxp240/ailes_legal_ai/data/raw/xml_judgments"
OUTPUT_FILE = "ailes_training_dataset.jsonl"
TRACKER_FILE = "ailes_tracker.xlsx"
CHECKPOINT_DIR = "ailes_checkpoints"
MANIFEST_FILE = "ailes_manifest.txt"
BATCH_SIZE = 5
USE_MISTRAL = True
USE_RAG = True  # Enable RAG for regenerative pairs
TARGET_PAIRS = 50  # Increased target
GENERATION_TIMEOUT = 60  # Increased timeout
MAX_MISTRAL_FAILURES = 5

TARGET_DISTRIBUTION = {
    "financial_processing": 0.30,
    "legal_reasoning": 0.25,
    "case_analysis": 0.20,
    "court_decision": 0.15,
    "conversational_guidance": 0.10
}

MISTRAL_MODEL_PATH = "/mnt/scratch/bgxp240/models/models--mistralai--Mistral-Nemo-Instruct-2407/snapshots/04d8a90549d23fc6bd7f642064003592df51e9b3/"

# Simplified prompt for better JSON generation
UNIFIED_PROMPT = """Extract legal training data from this UK family law judgment text.

Create 1-2 JSON objects with these exact fields:
- "task_type": choose from ["financial_processing", "legal_reasoning", "case_analysis", "court_decision", "conversational_guidance"]
- "instruction": what to ask the legal AI
- "response": what the AI should answer based on the text
- "summary": brief description

Base everything strictly on the provided text. Do not invent information.

TEXT:
{context}

Respond with only a JSON array:"""

class TimeoutError(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutError("Generation timed out")

def to_llama31_format(instruction: str, context: str, response: str) -> Dict:
    """Convert AILES data to Llama 3.1 format with special tokens"""
    BOS = "<|begin_of_text|>"
    SH = "<|start_header_id|>"
    EH = "<|end_header_id|>"
    EOT = "<|eot_id|>"
    
    system_prompt = "You are AILES, a specialized UK family law assistant."
    user_msg = f"{instruction}\n\n---\nJudgment excerpt:\n{context}"
    
    text = (
        f"{BOS}{SH}system{EH}\n\n{system_prompt}{EOT}"
        f"{SH}user{EH}\n\n{user_msg}{EOT}"
        f"{SH}assistant{EH}\n\n{response}{EOT}"
    )
    
    return {"text": text}

class StreamingJSONLWriter:
    """Streaming JSONL writer to prevent memory issues"""
    
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.file = None
        self.count = 0
    
    def __enter__(self):
        # Initialize count from existing file (if any) for proper resume
        existing = 0
        if os.path.exists(self.filepath):
            try:
                with open(self.filepath, 'r', encoding='utf-8', errors='ignore') as rf:
                    for _ in rf:
                        existing += 1
            except Exception:
                existing = 0
        
        self.file = open(self.filepath, 'a', encoding='utf-8')  # Append mode for safe resume
        self.count = existing
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.file:
            self.file.close()
    
    def write(self, record: Dict):
        """Write a record to the JSONL file"""
        if self.file:
            self.file.write(json.dumps(record, ensure_ascii=False) + '\n')
            self.file.flush()
            self.count += 1
    
    def get_count(self) -> int:
        return self.count

class RAGGenerator:
    """RAG system for generating regenerative pairs"""
    
    def __init__(self):
        self.embedder = None
        self.index = None
        self.corpus_texts = []
        self.initialized = False
    
    def initialize(self):
        """Initialize RAG components"""
        if not RAG_AVAILABLE:
            return False
        
        try:
            self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
            self.index = faiss.IndexFlatIP(384)  # Inner product for similarity
            self.initialized = True
            return True
        except Exception as e:
            print(f"Failed to initialize RAG: {e}")
            return False
    
    def add_to_corpus(self, text: str, metadata: Dict):
        """Add text to RAG corpus"""
        if not self.initialized:
            return
        
        try:
            embedding = self.embedder.encode([text], normalize_embeddings=True)
            self.index.add(embedding.astype('float32'))
            self.corpus_texts.append({"text": text, "metadata": metadata})
        except Exception as e:
            print(f"Failed to add to RAG corpus: {e}")
    
    def generate_regenerative_pairs(self, base_pair: Dict, context: str) -> List[Dict]:
        """Generate regenerative pairs based on similar content"""
        if not self.initialized or len(self.corpus_texts) < 10:
            return []
        
        try:
            # Generate query embedding
            query = f"{base_pair.get('instruction', '')} {base_pair.get('response', '')}"
            query_embedding = self.embedder.encode([query], normalize_embeddings=True)
            
            # Search for similar content
            scores, indices = self.index.search(query_embedding.astype('float32'), k=3)
            
            regenerative_pairs = []
            for score, idx in zip(scores[0], indices[0]):
                if score > 0.7 and idx < len(self.corpus_texts):  # High similarity threshold
                    similar_text = self.corpus_texts[idx]["text"]
                    
                    # Generate "what if" style question
                    task_type = base_pair.get("task_type", "case_analysis")
                    if task_type == "financial_processing":
                        instruction = f"What if the financial arrangements were different based on this similar case?"
                        response = f"Considering the similar case circumstances, alternative financial arrangements could include different asset division or maintenance provisions."
                    elif task_type == "legal_reasoning":
                        instruction = f"How would the legal reasoning differ in this alternative scenario?"
                        response = f"The legal reasoning would need to consider different statutory provisions and case law precedents."
                    else:
                        instruction = f"How would this case be decided differently in an alternative scenario?"
                        response = f"The case outcome could vary based on different facts and legal considerations."
                    
                    regenerative_pairs.append({
                        "task_type": task_type,
                        "instruction": instruction,
                        "response": response,
                        "summary": f"Regenerative analysis based on similar case"
                    })
            
            return regenerative_pairs[:1]  # Limit to 1 regenerative pair
            
        except Exception as e:
            print(f"Failed to generate regenerative pairs: {e}")
            return []

class AILESTracker:
    """Enhanced tracker with factuality logging"""
    
    def __init__(self, tracker_file: str = TRACKER_FILE, checkpoint_dir: str = CHECKPOINT_DIR):
        self.tracker_file = tracker_file
        self.checkpoint_dir = checkpoint_dir
        self.rows = []
        self.processed_files = set()
        self.stats = {
            "total_files": 0,
            "successful": 0,
            "failed": 0,
            "total_pairs": 0,
            "high_factuality": 0,
            "start_time": datetime.now()
        }
        
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.load_state()
    
    def load_state(self):
        """Load existing tracker state"""
        state_path = os.path.join(self.checkpoint_dir, "tracker_state.pkl")
        if os.path.exists(state_path):
            try:
                with open(state_path, 'rb') as f:
                    state = pickle.load(f)
                    self.rows = state.get('rows', [])
                    self.processed_files = state.get('processed_files', set())
                    self.stats = state.get('stats', self.stats)
                print(f"Loaded tracker state: {len(self.rows)} entries")
            except Exception as e:
                print(f"Failed to load tracker state: {e}")
    
    def save_state(self):
        """Save tracker state"""
        state_path = os.path.join(self.checkpoint_dir, "tracker_state.pkl")
        state = {
            'rows': self.rows,
            'processed_files': self.processed_files,
            'stats': self.stats
        }
        try:
            with open(state_path, 'wb') as f:
                pickle.dump(state, f)
        except Exception as e:
            print(f"Failed to save tracker state: {e}")
    
    def add_entry(self, file_name: str, pairs_count: int, factuality_scores: List[bool] = None, flagged_scores: List[bool] = None, error_msg: str = ""):
        """Add entry to tracker with factuality and flagging information"""
        if file_name in self.processed_files:
            return
        
        year_match = re.search(r'\[(\d{4})\]', file_name)
        year = year_match.group(1) if year_match else "unknown"
        
        status = "Failed" if error_msg else ("Empty" if pairs_count == 0 else "Success")
        
        # Calculate factuality metrics
        high_factuality_count = sum(factuality_scores) if factuality_scores else 0
        factuality_rate = (high_factuality_count / len(factuality_scores) * 100) if factuality_scores else 0
        factuality_score = "High" if factuality_rate >= 85 else "Medium" if factuality_rate >= 70 else "Low"
        
        # Calculate flagging metrics
        flagged_count = sum(flagged_scores) if flagged_scores else 0
        flagged_rate = (flagged_count / len(flagged_scores) * 100) if flagged_scores else 0
        
        row = {
            "File Name": file_name,
            "Year": year,
            "Status": status,
            "Pairs Generated": pairs_count,
            "Factuality Score": factuality_score,
            "Factuality Rate (%)": f"{factuality_rate:.1f}",
            "High Factuality Pairs": high_factuality_count,
            "Flagged for Review": flagged_count,
            "Flagged Rate (%)": f"{flagged_rate:.1f}",
            "Error": error_msg,
            "Processing Time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        self.rows.append(row)
        self.processed_files.add(file_name)
        
        # Update stats
        self.stats["total_files"] += 1
        if status == "Success":
            self.stats["successful"] += 1
        elif status == "Failed":
            self.stats["failed"] += 1
        
        self.stats["total_pairs"] += pairs_count
        self.stats["high_factuality"] += high_factuality_count
        if "flagged_pairs" not in self.stats:
            self.stats["flagged_pairs"] = 0
        self.stats["flagged_pairs"] += flagged_count
        self.save_state()
    
    def update_excel(self, force=False):
        """Update Excel file with enhanced metrics"""
        if len(self.rows) % 50 != 0 and not force:
            return
        
        try:
            df = pd.DataFrame(self.rows)
            with pd.ExcelWriter(self.tracker_file) as writer:
                df.to_excel(writer, sheet_name="Processing Log", index=False)
                
                # Add summary sheet with enhanced metrics
                if len(self.rows) > 0:
                    summary_data = {
                        "Metric": ["Total Files", "Successful", "Failed", "Total Pairs", "High Factuality Rate (%)", 
                                  "Flagged Rate (%)", "Avg Pairs/File"],
                        "Value": [
                            self.stats["total_files"],
                            self.stats["successful"], 
                            self.stats["failed"],
                            self.stats["total_pairs"],
                            f"{(self.stats['high_factuality'] / max(1, self.stats['total_pairs']) * 100):.1f}",
                            f"{(self.stats.get('flagged_pairs', 0) / max(1, self.stats['total_pairs']) * 100):.1f}",
                            f"{(self.stats['total_pairs'] / max(1, self.stats['successful'])):.1f}"
                        ]
                    }
                    summary_df = pd.DataFrame(summary_data)
                    summary_df.to_excel(writer, sheet_name="Summary", index=False)
        except Exception as e:
            print(f"Failed to update Excel: {e}")
    
    def print_status(self):
        """Print enhanced status with flagging metrics"""
        runtime = datetime.now() - self.stats["start_time"]
        rate = self.stats["total_files"] / max(0.1, runtime.total_seconds() / 3600)
        factuality_rate = (self.stats["high_factuality"] / max(1, self.stats["total_pairs"]) * 100)
        flagged_rate = (self.stats.get("flagged_pairs", 0) / max(1, self.stats["total_pairs"]) * 100)
        
        print(f"\n{'='*60}")
        print(f"Files: {self.stats['total_files']} | Success: {self.stats['successful']} | Pairs: {self.stats['total_pairs']}")
        print(f"Runtime: {runtime.total_seconds()/3600:.1f}h | Rate: {rate:.1f} files/h")
        print(f"Factuality: {factuality_rate:.1f}% high quality")
        print(f"Flagged for review: {flagged_rate:.1f}% ({self.stats.get('flagged_pairs', 0)} pairs)")
        print(f"{'='*60}")

def setup_logging():
    """Setup logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('ailes_pipeline.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def strip_namespace(tag: str) -> str:
    """Remove XML namespace from tag"""
    return tag.split('}', 1)[-1] if '}' in tag else tag

def enhanced_xml_parser(file_path: str, logger) -> Dict[str, Any]:
    """Robust XML parsing with lxml and namespace handling"""
    try:
        with open(file_path, 'rb') as f:
            parser = LET.XMLParser(recover=True, remove_blank_text=True)
            root = LET.parse(f, parser).getroot()
        
        # Extract metadata
        metadata = {}
        for elem in root.iter():
            tag = strip_namespace(elem.tag).lower()
            if 'citation' in tag and elem.text and elem.text.strip():
                metadata['citation'] = elem.text.strip()
                break
        
        # Extract paragraphs with better text extraction
        paragraphs = []
        for elem in root.iter():
            tag = strip_namespace(elem.tag).lower()
            if tag in ['p', 'para', 'paragraph', 'level']:
                raw = ''.join(elem.itertext())  # Get all text including children
                text = ' '.join(raw.split())
                if len(text) > 50:
                    paragraphs.append(text)
        
        # Fallback: if no standard paragraphs found, extract from document containers
        if not paragraphs:
            for xpath in [".//body", ".//judgment", ".//text"]:
                try:
                    for elem in root.xpath(xpath):
                        raw = ''.join(elem.itertext())
                        text = ' '.join(raw.split())
                        if len(text) > 200:
                            paragraphs.append(text)
                except:
                    continue  # Skip if xpath fails
        
        # Improved chunking - preserve paragraph boundaries
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            # Check if adding this paragraph would exceed limit
            if len(current_chunk) + len(para) + 2 > 3000 and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                current_chunk = current_chunk + "\n\n" + para if current_chunk else para
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # Filter very short chunks
        chunks = [c for c in chunks if len(c) > 200]
        
        total_chars = sum(len(c) for c in chunks)
        logger.info(f"Parsed {os.path.basename(file_path)}: {len(chunks)} chunks, {total_chars} chars")
        
        return {
            "metadata": metadata,
            "chunks": chunks,
            "char_count": total_chars,
            "chunk_count": len(chunks),
            "file_name": os.path.basename(file_path),
            "full_text": "\n\n".join(paragraphs)
        }
        
    except Exception as e:
        logger.error(f"Error parsing {file_path}: {str(e)}")
        return {"error": str(e), "file_name": os.path.basename(file_path)}

def load_mistral_model():
    """Load Mistral model with improved settings"""
    if not DEPENDENCIES_AVAILABLE:
        return None, None, None
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(MISTRAL_MODEL_PATH)
        tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            MISTRAL_MODEL_PATH,
            torch_dtype=torch.bfloat16,  # More stable than float16
            device_map="cuda",
            low_cpu_mem_usage=True
        )
        
        # Get context length and set generation config
        max_ctx = getattr(model.config, "max_position_embeddings", 8192)
        model.generation_config.pad_token_id = tokenizer.eos_token_id
        
        print(f"Mistral ctx window: {max_ctx}")
        print(f"EOS id: {tokenizer.eos_token_id}, pad id: {model.generation_config.pad_token_id}")
        
        return model, tokenizer, max_ctx
    except Exception as e:
        print(f"Error loading Mistral model: {e}")
        return None, None, None

def repair_json(response: str) -> str:
    """Repair common JSON issues"""
    try:
        json.loads(response)
        return response
    except json.JSONDecodeError:
        # Attempt to fix common issues
        response = response.strip()
        if not response.startswith('['):
            response = '[' + response
        if not response.endswith(']'):
            response = response + ']'
        # Replace incomplete objects
        response = re.sub(r',\s*}', '}', response)
        response = re.sub(r',\s*\]', ']', response)
        try:
            json.loads(response)
            return response
        except json.JSONDecodeError:
            return '[]'

def generate_with_mistral_safe(model, tokenizer, prompt: str, max_ctx: int, logger=None) -> List[Dict]:
    """Enhanced generation with response validation and JSON repair"""
    if model is None or tokenizer is None:
        if logger:
            logger.warning("MISTRAL DEBUG: Model or tokenizer is None - using fallback")
        return []
    
    try:
        if logger:
            logger.info(f"MISTRAL DEBUG: Attempting generation with prompt length: {len(prompt)}")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(GENERATION_TIMEOUT)
        
        try:
            messages = [
                {"role": "system", "content": "Extract legal training data as JSON array only."},
                {"role": "user", "content": prompt}
            ]
            formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            if logger:
                logger.info(f"MISTRAL DEBUG: Using chat template, formatted length: {len(formatted_prompt)}")
        except:
            formatted_prompt = prompt
            if logger:
                logger.info("MISTRAL DEBUG: Chat template failed, using simple prompt")
        
        inputs = tokenizer(formatted_prompt, return_tensors="pt", add_special_tokens=False)
        input_length = inputs.input_ids.shape[-1]
        
        min_gen = 256
        buffer = 100
        if input_length + min_gen + buffer > max_ctx:
            max_prompt_tokens = max(min_gen + buffer, max_ctx - (min_gen + buffer))
            inputs = tokenizer(formatted_prompt, return_tensors="pt", 
                             truncation=True, max_length=max_prompt_tokens, add_special_tokens=False)
            input_length = inputs.input_ids.shape[-1]
            if logger:
                logger.info(f"MISTRAL DEBUG: Truncated prompt to {input_length} tokens")
        
        max_new_tokens = max(min_gen, min(2000, max_ctx - input_length - buffer))  # Increased
        if logger:
            logger.info(f"MISTRAL DEBUG: Input tokens: {input_length}, Max new tokens: {max_new_tokens}")
        
        inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                temperature=None,
                top_p=None,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id
            )
        
        signal.alarm(0)
        
        response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
        
        if logger:
            logger.info(f"MISTRAL DEBUG: Raw response length: {len(response)}")
            logger.info(f"MISTRAL DEBUG: Raw response preview: {response[:500]}...")
        
        # Repair JSON
        response = repair_json(response)
        
        try:
            pairs = json.loads(response)
            if isinstance(pairs, dict):
                pairs = [pairs]
            
            # Enhanced validation with response field check
            valid_pairs = []
            for pair in pairs:
                if isinstance(pair, dict) and all(k in pair for k in ["task_type", "instruction", "response", "summary"]):
                    # Ensure response field is not empty
                    if "response" not in pair or not pair["response"].strip():
                        if logger:
                            logger.warning(f"Missing or empty response in pair: {pair}")
                        continue
                    valid_pairs.append(pair)
                else:
                    if logger:
                        logger.warning(f"MISTRAL DEBUG: Invalid pair missing required fields: {pair}")
            
            if logger:
                logger.info(f"MISTRAL DEBUG: Successfully parsed {len(valid_pairs)} valid pairs from {len(pairs)} total")
            
            return valid_pairs
            
        except json.JSONDecodeError as e:
            if logger:
                logger.warning(f"MISTRAL DEBUG: JSON parsing failed: {e}. Raw response: {response[:500]}...")
            return []
            
    except TimeoutError:
        if logger:
            logger.warning("MISTRAL DEBUG: Generation timed out")
        return []
    except Exception as e:
        if logger:
            logger.error(f"MISTRAL DEBUG: Generation failed with error: {e}")
        return []
    finally:
        signal.alarm(0)

def validate_factuality_fast(pair: Dict, full_text: str, threshold=0.65) -> bool:
    """Fast factuality validation using word overlap"""
    try:
        response = pair.get("response", "")
        if len(response) < 30:
            return True
        
        # Simple validation - check if key phrases from response exist in source
        response_lower = response.lower()
        full_text_lower = full_text.lower()
        
        # Extract first few sentences
        sentences = [s.strip() for s in re.split(r'[.!?]', response) if len(s.strip()) > 10][:3]
        
        for sentence in sentences:
            sentence_lower = sentence.lower().strip()
            
            # Direct substring match
            if sentence_lower in full_text_lower:
                continue
            
            # Word overlap validation
            words = sentence_lower.split()
            if len(words) > 3:
                # Check if most words appear in the text
                word_matches = sum(1 for word in words if len(word) > 3 and word in full_text_lower)
                word_ratio = word_matches / len(words)
                if word_ratio >= threshold:
                    continue
            
            # If no good match found, fail validation
            return False
        
        return True
        
    except Exception:
        return True

def generate_fallback_pairs(chunk: str, metadata: Dict) -> List[Dict]:
    """Enhanced rule-based fallback generation"""
    pairs = []
    chunk_lower = chunk.lower()
    
    # Financial fallback
    if any(term in chunk_lower for term in ["financial", "assets", "income", "maintenance", "property", "ancillary"]):
        pairs.append({
            "task_type": "financial_processing",
            "instruction": "What financial arrangements are discussed in this family law judgment?",
            "response": "The judgment discusses financial matters including property, income, assets, and maintenance arrangements between the parties.",
            "summary": "Financial arrangements extraction"
        })
    
    # Legal reasoning fallback
    if any(term in chunk_lower for term in ["section", "act", "statute", "law", "principle", "test"]):
        pairs.append({
            "task_type": "legal_reasoning",
            "instruction": "What legal principles and statutory provisions are applied in this case?",
            "response": "The judgment applies relevant statutory provisions and established legal principles from family law jurisprudence.",
            "summary": "Legal reasoning analysis"
        })
    
    # Court decision fallback
    if any(term in chunk_lower for term in ["order", "direct", "grant", "dismiss", "judgment", "declare"]):
        pairs.append({
            "task_type": "court_decision",
            "instruction": "What orders and directions did the court make?",
            "response": "The court made specific orders and directions as detailed in the judgment to resolve the matters before it.",
            "summary": "Court orders extraction"
        })
    
    # Conversational guidance fallback
    if any(term in chunk_lower for term in ["advice", "consider", "guidance", "recommend"]):
        pairs.append({
            "task_type": "conversational_guidance",
            "instruction": "What guidance can be provided based on this family law case?",
            "response": "Based on this case, parties should consider the legal principles and precedents established in similar family law matters.",
            "summary": "Legal guidance extraction"
        })
    
    return pairs

def flag_sensitive_pair(pair: Dict, context: str = "") -> Dict:
    """Flag pairs with potentially sensitive content for review"""
    sensitive_terms = ["child abuse", "domestic violence", "sexual abuse", "abduction", "forced marriage", 
                      "trafficking", "female genital mutilation", "rape", "assault"]
    pair_text = f"{pair.get('instruction', '')} {pair.get('response', '')} {context}".lower()
    pair["flagged_for_review"] = any(term in pair_text for term in sensitive_terms)
    return pair

def generate_training_pairs(parsed_data: Dict[str, Any], model=None, tokenizer=None, max_ctx=None, logger=None, rag_generator=None) -> List[Tuple[Dict[str, Any], str, bool, bool]]:
    """Enhanced training pair generation with increased yield, RAG, and ethical flagging - returns (pair, source_chunk, factuality, flagged) tuples"""
    if "error" in parsed_data:
        return []
    
    pairs = []
    chunks = parsed_data["chunks"]
    metadata = parsed_data["metadata"]
    full_text = parsed_data.get("full_text", "")
    
    mistral_failure_count = 0
    
    # Increased chunk processing for higher yield
    max_chunks = min(20, len(chunks))  # Increased from 10 to 20
    
    for i, chunk in enumerate(chunks[:max_chunks]):
        if len(chunk.strip()) < 100:
            continue
        
        # Process full chunks to prevent truncation
        processing_chunk = chunk  # No truncation
        source_chunk = chunk
        
        generated_pairs = []
        
        # Try Mistral generation
        if USE_MISTRAL and model is not None and mistral_failure_count < MAX_MISTRAL_FAILURES:
            if logger:
                logger.info(f"MISTRAL DEBUG: Attempting Mistral generation for chunk {i+1}/{max_chunks}")
            
            prompt = UNIFIED_PROMPT.format(context=processing_chunk)
            mistral_pairs = generate_with_mistral_safe(model, tokenizer, prompt, max_ctx, logger)
            
            if mistral_pairs:
                generated_pairs = mistral_pairs
                if logger:
                    logger.info(f"MISTRAL DEBUG: SUCCESS - Got {len(mistral_pairs)} pairs from Mistral for chunk {i+1}")
            else:
                mistral_failure_count += 1
                if logger:
                    logger.warning(f"MISTRAL DEBUG: FAILED - Using fallback. Failure count: {mistral_failure_count}/{MAX_MISTRAL_FAILURES}")
        
        # Fallback to rule-based if Mistral fails
        if not generated_pairs:
            if logger:
                logger.info(f"MISTRAL DEBUG: Using rule-based fallback generation for chunk {i+1}")
            generated_pairs = generate_fallback_pairs(processing_chunk, metadata)
        
        # Add to RAG corpus if available
        if rag_generator:
            rag_generator.add_to_corpus(processing_chunk, metadata)
        
        # Convert to final format and validate
        for gp in generated_pairs:
            # Flag sensitive content
            gp = flag_sensitive_pair(gp, source_chunk)
            if gp.get("flagged_for_review"):
                if logger:
                    logger.warning(f"Flagged pair for review: {gp['task_type']} from {parsed_data['file_name']}")
            
            # Validate factuality
            factuality_valid = validate_factuality_fast(gp, full_text) if full_text else True
            if not factuality_valid and logger:
                logger.warning(f"Low factuality pair: {gp['task_type']} from {parsed_data['file_name']}")
            
            pairs.append((gp, source_chunk, factuality_valid, gp.get("flagged_for_review", False)))  # Return enhanced tuple
    
    # Generate RAG pairs if enabled
    if USE_RAG and rag_generator and len(pairs) > 0:
        for (base_pair, base_chunk, _, _) in pairs[:2]:  # Use first 2 pairs as base
            regenerative_pairs = rag_generator.generate_regenerative_pairs(base_pair, base_chunk)
            for rp in regenerative_pairs:
                rp = flag_sensitive_pair(rp, base_chunk)
                pairs.append((rp, base_chunk, True, rp.get("flagged_for_review", False)))  # RAG pairs with flagging
    
    if logger:
        flagged_count = sum(1 for _, _, _, flagged in pairs if flagged)
        logger.info(f"Generated {len(pairs)} pairs for {parsed_data['file_name']} ({flagged_count} flagged for review)")
    return pairs

def accept_by_quota(task_type: str, task_counts: Counter, total_accepted: int) -> bool:
    """Enhanced quota check with relaxation for under-represented tasks"""
    target_ratio = TARGET_DISTRIBUTION.get(task_type, 0.0)
    if target_ratio == 0:
        return False
    
    # Relax quota for missing task types after 100 total pairs
    if task_counts[task_type] == 0 and total_accepted > 100:
        return True  # Allow first pair for under-represented tasks
    
    # Calculate what proportion this task would have if we accept one more
    new_proportion = (task_counts[task_type] + 1) / max(1, total_accepted + 1)
    
    # Accept if we're under the target ratio (with buffer)
    return new_proportion <= target_ratio + 0.05

def process_batch(xml_files: List[str], batch_num: int, model=None, tokenizer=None, max_ctx=None, 
                 logger=None, tracker=None, writer=None, task_counts=None, rag_generator=None) -> int:
    """Enhanced batch processing with factuality and flagging tracking"""
    logger.info(f"Processing batch {batch_num}: {len(xml_files)} files")
    batch_pairs_count = 0
    
    for file_name in xml_files:
        if tracker and file_name in tracker.processed_files:
            continue
        
        start_time = time.time()
        file_path = os.path.join(CORPUS_DIR, file_name)
        
        try:
            parsed_data = enhanced_xml_parser(file_path, logger)
            
            if "error" in parsed_data:
                if tracker:
                    tracker.add_entry(file_name, 0, [], [], parsed_data["error"])
                continue
            
            pairs = generate_training_pairs(parsed_data, model, tokenizer, max_ctx, logger, rag_generator)
            
            # Process pairs with quota management, factuality and flagging tracking
            file_pairs_count = 0
            factuality_scores = []
            flagged_scores = []
            
            for (pair, source_chunk, factuality_valid, flagged) in pairs:  # Unpack enhanced tuple
                task_type = pair.get("task_type", "case_analysis")
                
                # Check quota with relaxation
                if accept_by_quota(task_type, task_counts, writer.get_count()):
                    # Convert to Llama 3.1 format using exact source chunk
                    record = to_llama31_format(
                        instruction=pair.get("instruction", "Analyze this legal text"),
                        context=source_chunk,
                        response=pair.get("response", "")
                    )
                    
                    # Debug: Log format conversion for first pair
                    if logger and file_pairs_count == 0:
                        logger.info(f"FORMAT DEBUG: Converting pair to Llama 3.1 format")
                        logger.info(f"FORMAT DEBUG: Input - instruction: {pair.get('instruction', '')[:100]}...")
                        logger.info(f"FORMAT DEBUG: Input - response: {pair.get('response', '')[:100]}...")
                        logger.info(f"FORMAT DEBUG: Output preview: {record['text'][:200]}...")
                    
                    # Write to streaming output
                    writer.write(record)
                    task_counts[task_type] += 1
                    file_pairs_count += 1
                    batch_pairs_count += 1
                    factuality_scores.append(factuality_valid)
                    flagged_scores.append(flagged)
                else:
                    # Log quota rejection
                    current_proportion = task_counts[task_type] / max(1, writer.get_count())
                    target_proportion = TARGET_DISTRIBUTION.get(task_type, 0)
                    if logger and current_proportion > target_proportion:
                        logger.debug(f"Quota exceeded for {task_type}: {current_proportion:.2f} > {target_proportion:.2f}")
            
            processing_time = time.time() - start_time
            logger.info(f"Generated {file_pairs_count} pairs for {file_name} in {processing_time:.1f}s")
            
            if tracker:
                tracker.add_entry(file_name, file_pairs_count, factuality_scores, flagged_scores)
            
            # Update manifest once per file
            try:
                with open(MANIFEST_FILE, 'a', encoding='utf-8') as mf:
                    mf.write(parsed_data["file_name"] + '\n')
            except Exception as e:
                logger.warning(f"Failed to update manifest for {file_name}: {e}")
                
        except Exception as e:
            logger.error(f"Failed to process {file_name}: {e}")
            if tracker:
                tracker.add_entry(file_name, 0, [], [], str(e))
    
    return batch_pairs_count

def main():
    """Enhanced main pipeline with RAG and improved metrics"""
    # Set seeds for reproducibility
    random.seed(42)
    os.environ["PYTHONHASHSEED"] = "42"
    if DEPENDENCIES_AVAILABLE:
        torch.manual_seed(42)
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True
    
    logger = setup_logging()
    logger.info("Starting Enhanced AILES Pipeline with Llama 3.1 Format and RAG")
    
    if not DEPENDENCIES_AVAILABLE:
        logger.error("Missing required dependencies. Please install: lxml, torch, transformers")
        return
    
    tracker = AILESTracker()
    
    if not os.path.exists(CORPUS_DIR):
        logger.error(f"Corpus directory not found: {CORPUS_DIR}")
        return
    
    xml_files = [f for f in os.listdir(CORPUS_DIR) if f.endswith('.xml')]
    if not xml_files:
        logger.error(f"No XML files found")
        return
    
    xml_files = xml_files[:10]
    
    logger.info(f"Found {len(xml_files)} XML files (limited to 10 for testing)")
    
    # Filter out already processed files
    remaining_files = [f for f in xml_files if f not in tracker.processed_files]
    
    # Load manifest for additional resume safety
    if os.path.exists(MANIFEST_FILE):
        try:
            with open(MANIFEST_FILE, 'r', encoding='utf-8') as mf:
                manifest_files = {line.strip() for line in mf if line.strip()}
                tracker.processed_files.update(manifest_files)
                remaining_files = [f for f in remaining_files if f not in manifest_files]
                logger.info(f"Loaded manifest with {len(manifest_files)} processed files")
        except Exception as e:
            logger.warning(f"Failed to load manifest: {e}")
    
    random.shuffle(remaining_files)
    logger.info(f"Processing {len(remaining_files)} remaining files")
    
    # Initialize RAG generator
    rag_generator = None
    if USE_RAG:
        rag_generator = RAGGenerator()
        if rag_generator.initialize():
            logger.info("RAG generator initialized successfully")
        else:
            logger.warning("Failed to initialize RAG generator")
            rag_generator = None
    
    # Load model
    model, tokenizer, max_ctx = None, None, 8192
    if USE_MISTRAL:
        logger.info("Loading Mistral model...")
        model, tokenizer, max_ctx = load_mistral_model()
        if model is None:
            logger.warning("Failed to load Mistral model, using fallback only")
        else:
            # Test Mistral generation
            logger.info("Testing Mistral generation...")
            test_prompt = "Extract legal data from: 'The court ordered maintenance of Â£500 per month.'"
            test_result = generate_with_mistral_safe(model, tokenizer, test_prompt, max_ctx, logger)
            logger.info(f"MISTRAL TEST: {len(test_result)} pairs generated in test")
            if test_result:
                logger.info(f"MISTRAL TEST SUCCESS: {test_result[0]}")
            else:
                logger.error("MISTRAL TEST FAILED: No pairs generated")
    
    # Configuration check
    logger.info(f"CONFIG CHECK: USE_MISTRAL = {USE_MISTRAL}")
    logger.info(f"CONFIG CHECK: USE_RAG = {USE_RAG}")
    logger.info(f"CONFIG CHECK: Model loaded = {model is not None}")
    logger.info(f"CONFIG CHECK: Target pairs = {TARGET_PAIRS}")
    
    # Initialize task counter for quota management
    task_counts = Counter()
    
    try:
        # Process with streaming output
        with StreamingJSONLWriter(OUTPUT_FILE) as writer:
            # Process in batches
            for i in range(0, len(remaining_files), BATCH_SIZE):
                batch_files = remaining_files[i:i + BATCH_SIZE]
                batch_num = (i // BATCH_SIZE) + 1
                
                batch_count = process_batch(
                    batch_files, batch_num, model, tokenizer, max_ctx, 
                    logger, tracker, writer, task_counts, rag_generator
                )
                
                # Log task distribution warnings
                total_pairs = writer.get_count()
                if total_pairs > 100:
                    for task_type, target_ratio in TARGET_DISTRIBUTION.items():
                        if task_counts[task_type] == 0:
                            logger.warning(f"No pairs for {task_type}; relaxing quota")
                
                # Update Excel tracker periodically
                tracker.update_excel()
                tracker.print_status()
                
                # Check if we've reached target
                if writer.get_count() >= TARGET_PAIRS:
                    logger.info(f"Reached target of {TARGET_PAIRS} pairs")
                    break
                
                # Memory cleanup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    
    except KeyboardInterrupt:
        logger.info("Interrupted by user")
        return
    
    # Final updates
    tracker.update_excel(force=True)
    
    # Print final enhanced distribution
    logger.info("="*60)
    logger.info("PIPELINE COMPLETE")
    logger.info(f"Total pairs generated: {sum(task_counts.values())}")
    logger.info(f"Files processed: {len(tracker.processed_files)}")
    logger.info(f"Success rate: {tracker.stats['successful']/max(1,tracker.stats['total_files'])*100:.1f}%")
    
    logger.info("\nTask distribution:")
    total = sum(task_counts.values())
    for task_type, count in task_counts.most_common():
        percentage = (count / total * 100) if total > 0 else 0
        target_pct = TARGET_DISTRIBUTION.get(task_type, 0) * 100
        logger.info(f"  {task_type}: {count} ({percentage:.1f}%, target: {target_pct:.1f}%)")
    
    # Log factuality and flagging summary
    factuality_rate = (tracker.stats["high_factuality"] / max(1, tracker.stats["total_pairs"]) * 100)
    flagged_rate = (tracker.stats.get("flagged_pairs", 0) / max(1, tracker.stats["total_pairs"]) * 100)
    logger.info(f"\nQuality Summary:")
    logger.info(f"  High factuality pairs: {tracker.stats['high_factuality']}/{tracker.stats['total_pairs']} ({factuality_rate:.1f}%)")
    logger.info(f"  Flagged for review: {tracker.stats.get('flagged_pairs', 0)}/{tracker.stats['total_pairs']} ({flagged_rate:.1f}%)")
    
    logger.info(f"\nDataset saved to: {OUTPUT_FILE}")
    logger.info(f"Tracker saved to: {TRACKER_FILE}")
    logger.info("Ready for Llama 3.1 8B fine-tuning!")
    logger.info("="*60)

if __name__ == "__main__":
    main()